{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Repository Link](https://github.com/CharlyMech/IEDIB_CEIA_2024-25/blob/main/Modelos%20de%20Inteligencia%20Artificial/Bloque%205/Actividades/5.1/tarea_5_Carlos_SanchezRecio.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ca_core_news_sm@ https://github.com/explosion/spacy-models/releases/download/ca_core_news_sm-3.8.0/ca_core_news_sm-3.8.0-py3-none-any.whl#sha256=e214211aa8da91c24ebdc453c2aa5f54fac09f44e01e65bcbdd3b0a5cb94d809\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ca_core_news_sm-3.8.0/ca_core_news_sm-3.8.0-py3-none-any.whl (19.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting es_core_news_sm@ https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: annotated-types==0.7.0 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: appnope==0.1.4 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (0.1.4)\n",
      "Requirement already satisfied: asttokens==3.0.0 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (3.0.0)\n",
      "Requirement already satisfied: beautifulsoup4==4.13.3 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (4.13.3)\n",
      "Requirement already satisfied: blis==1.2.0 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: bs4==0.0.2 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.0.2)\n",
      "Requirement already satisfied: catalogue==2.0.10 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (2.0.10)\n",
      "Requirement already satisfied: certifi==2025.1.31 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer==3.4.1 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (3.4.1)\n",
      "Requirement already satisfied: click==8.1.8 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (8.1.8)\n",
      "Requirement already satisfied: cloudpathlib==0.20.0 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (0.20.0)\n",
      "Requirement already satisfied: comm==0.2.2 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (0.2.2)\n",
      "Requirement already satisfied: confection==0.1.5 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (0.1.5)\n",
      "Requirement already satisfied: cymem==2.0.11 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 15)) (2.0.11)\n",
      "Requirement already satisfied: debugpy==1.8.12 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 16)) (1.8.12)\n",
      "Requirement already satisfied: decorator==5.1.1 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 17)) (5.1.1)\n",
      "Requirement already satisfied: exceptiongroup==1.2.2 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (1.2.2)\n",
      "Requirement already satisfied: executing==2.2.0 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 20)) (2.2.0)\n",
      "Requirement already satisfied: filelock==3.17.0 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 21)) (3.17.0)\n",
      "Requirement already satisfied: fsspec==2025.2.0 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 22)) (2025.2.0)\n",
      "Requirement already satisfied: huggingface-hub==0.28.1 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 23)) (0.28.1)\n",
      "Requirement already satisfied: idna==3.10 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 24)) (3.10)\n",
      "Requirement already satisfied: ipykernel==6.29.5 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 25)) (6.29.5)\n",
      "Requirement already satisfied: ipython==8.32.0 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 26)) (8.32.0)\n",
      "Requirement already satisfied: jedi==0.19.2 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 27)) (0.19.2)\n",
      "Requirement already satisfied: Jinja2==3.1.5 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 28)) (3.1.5)\n",
      "Requirement already satisfied: joblib==1.4.2 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 29)) (1.4.2)\n",
      "Requirement already satisfied: jupyter_client==8.6.3 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 30)) (8.6.3)\n",
      "Requirement already satisfied: jupyter_core==5.7.2 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 31)) (5.7.2)\n",
      "Requirement already satisfied: langcodes==3.5.0 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 32)) (3.5.0)\n",
      "Requirement already satisfied: language_data==1.3.0 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 33)) (1.3.0)\n",
      "Requirement already satisfied: marisa-trie==1.2.1 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 34)) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py==3.0.0 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 35)) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe==3.0.2 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 36)) (3.0.2)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.7 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 37)) (0.1.7)\n",
      "Requirement already satisfied: mdurl==0.1.2 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 38)) (0.1.2)\n",
      "Requirement already satisfied: mpmath==1.3.0 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 39)) (1.3.0)\n",
      "Requirement already satisfied: murmurhash==1.0.12 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 40)) (1.0.12)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 41)) (1.6.0)\n",
      "Requirement already satisfied: networkx==3.4.2 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 42)) (3.4.2)\n",
      "Requirement already satisfied: numpy==2.2.2 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 43)) (2.2.2)\n",
      "Requirement already satisfied: packaging==24.2 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 44)) (24.2)\n",
      "Requirement already satisfied: parso==0.8.4 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 45)) (0.8.4)\n",
      "Requirement already satisfied: pexpect==4.9.0 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 46)) (4.9.0)\n",
      "Requirement already satisfied: pillow==11.1.0 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 47)) (11.1.0)\n",
      "Requirement already satisfied: platformdirs==4.3.6 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 48)) (4.3.6)\n",
      "Requirement already satisfied: preshed==3.0.9 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 49)) (3.0.9)\n",
      "Requirement already satisfied: prompt_toolkit==3.0.50 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 50)) (3.0.50)\n",
      "Requirement already satisfied: psutil==6.1.1 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 51)) (6.1.1)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 52)) (0.7.0)\n",
      "Requirement already satisfied: pure_eval==0.2.3 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 53)) (0.2.3)\n",
      "Requirement already satisfied: pydantic==2.10.6 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 54)) (2.10.6)\n",
      "Requirement already satisfied: pydantic_core==2.27.2 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 55)) (2.27.2)\n",
      "Requirement already satisfied: Pygments==2.19.1 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 56)) (2.19.1)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 57)) (2.9.0.post0)\n",
      "Requirement already satisfied: PyYAML==6.0.2 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 58)) (6.0.2)\n",
      "Requirement already satisfied: pyzmq==26.2.1 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 59)) (26.2.1)\n",
      "Requirement already satisfied: qpsolvers==4.4.0 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 60)) (4.4.0)\n",
      "Requirement already satisfied: regex==2024.11.6 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 61)) (2024.11.6)\n",
      "Requirement already satisfied: requests==2.32.3 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 62)) (2.32.3)\n",
      "Requirement already satisfied: rich==13.9.4 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 63)) (13.9.4)\n",
      "Requirement already satisfied: safetensors==0.5.2 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 64)) (0.5.2)\n",
      "Requirement already satisfied: scikit-learn==1.6.1 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 65)) (1.6.1)\n",
      "Requirement already satisfied: scipy==1.15.1 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 66)) (1.15.1)\n",
      "Requirement already satisfied: sentence-transformers==3.4.1 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 67)) (3.4.1)\n",
      "Requirement already satisfied: shellingham==1.5.4 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 68)) (1.5.4)\n",
      "Requirement already satisfied: six==1.17.0 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 69)) (1.17.0)\n",
      "Requirement already satisfied: smart-open==7.1.0 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 70)) (7.1.0)\n",
      "Requirement already satisfied: soupsieve==2.6 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 71)) (2.6)\n",
      "Requirement already satisfied: spacy==3.8.4 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 72)) (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy==3.0.12 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 73)) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers==1.0.5 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 74)) (1.0.5)\n",
      "Requirement already satisfied: srsly==2.5.1 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 75)) (2.5.1)\n",
      "Requirement already satisfied: stack-data==0.6.3 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 76)) (0.6.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 77)) (1.13.1)\n",
      "Requirement already satisfied: thinc==8.3.4 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 78)) (8.3.4)\n",
      "Requirement already satisfied: threadpoolctl==3.5.0 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 79)) (3.5.0)\n",
      "Requirement already satisfied: tokenizers==0.21.0 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 80)) (0.21.0)\n",
      "Requirement already satisfied: torch==2.6.0 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 81)) (2.6.0)\n",
      "Requirement already satisfied: tornado==6.4.2 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 82)) (6.4.2)\n",
      "Requirement already satisfied: tqdm==4.67.1 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 83)) (4.67.1)\n",
      "Requirement already satisfied: traitlets==5.14.3 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 84)) (5.14.3)\n",
      "Requirement already satisfied: transformers==4.48.2 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 85)) (4.48.2)\n",
      "Requirement already satisfied: typer==0.15.1 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 86)) (0.15.1)\n",
      "Requirement already satisfied: typing_extensions==4.12.2 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 87)) (4.12.2)\n",
      "Requirement already satisfied: urllib3==2.3.0 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 88)) (2.3.0)\n",
      "Requirement already satisfied: wasabi==1.1.3 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 89)) (1.1.3)\n",
      "Requirement already satisfied: wcwidth==0.2.13 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 90)) (0.2.13)\n",
      "Requirement already satisfied: weasel==0.4.1 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 91)) (0.4.1)\n",
      "Requirement already satisfied: wrapt==1.17.2 in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 92)) (1.17.2)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from marisa-trie==1.2.1->-r requirements.txt (line 34)) (65.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la tarea de esta entrega trabajará cuatro aplicaciones de procesamiento del lenguaje general: clasificación de texto, generación de texto, análisis sintáctico y recuperación de información (_Information Retrieval_) . \n",
    "\n",
    "Las funciones que necesitará las puede obtener del código del capítulo de NLP del libro AIMA. Deberá mirar qué dependencias tienen respecto a otros archivos, incluir el código que sea necesario o importarlo de otros módulos.\n",
    "\n",
    "[https://github.com/aimacode/aima-python/blob/master/nlp_apps.ipynb](https://github.com/aimacode/aima-python/blob/master/nlp_apps.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Clasificación de texto según el autor\n",
    "\n",
    "Construya dos modelos de diferentes autores: Josep Carner y Miquel dels Sants Oliver. Para eso, puede usar sus obras disponibles en el Proyecto Gutenberg. Por ejemplo, de Carner tome la traducción de los cuentos de Mark Twain. Después, clasifica frases en el estilo de cada uno que muestren cómo su modelo las identifica correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aima_module.utils import open_data\n",
    "from aima_module.text import words, UnigramWordModel\n",
    "from aima_module.learning import NaiveBayesLearner\n",
    "\n",
    "def recognize_author(text, nbs):\n",
    "   return nbs(words(text.lower()))\n",
    "\n",
    "hostel = open_data('CA-Text/bolla_hostel.txt').read()\n",
    "wordseq_hostel = words(hostel)\n",
    "p_miquel = UnigramWordModel(wordseq_hostel, 5)\n",
    "\n",
    "adventures = open_data('CA-Text/tom_sawyer_adventures.txt').read()\n",
    "wordseq_adventures = words(adventures)\n",
    "p_josep = UnigramWordModel(wordseq_adventures, 5)\n",
    "\n",
    "\n",
    "nbs = NaiveBayesLearner({('Miquel', 1): p_miquel, ('Josep', 1): p_josep}, simple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Miquel'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From the \"L'hostal de la Bolla\" [line 172]\n",
    "recognize_author('de la familia, entre sacs de bessó i senalles de llegum. La botiga,', nbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Josep'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From \"The Adventures of Tom Sawyer\" [line 4149]\n",
    "recognize_author('resta de cent yardes. Nedava quartejant per amunt el corrent, però', nbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Author's quotes prompt](./authors_phrases_prompt.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Josep'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quote from Miquel dels Sants Oliver\n",
    "recognize_author('La joventut, en el sentit màgic, potser no ha mai existit i potser no és sinó un equador convencional entre la il·lusió i el record.', nbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Josep'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quote from Josep Carner\n",
    "recognize_author('Una mica d\\'amistat fins a morir diu que dura. Vés, amor, metzina impura; vull la mel, vull el brossat d\\'una mica d\\'amistat.', nbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como comentario a cerca del resultado, se puede observar que no es del todo preciso en la clasificación. Haciendo uso de la actividad de mi compañero David Ramírez ([enlace a su tarea](https://github.com/davidzz-code/IEDIB_2024_2025/blob/main/02%20Modelos%20IA/T5%20NLP/tarea_4_David_Ramirez_Ruiz.ipynb)), voy a probar con las frases que ha utilizado él."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Miquel'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quote from Miquel dels Sants Oliver\n",
    "recognize_author('Els carrers, encara humits, semblaven desertar de la nit', nbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Josep'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quote from Josep Carner\n",
    "recognize_author('Amb un gest majestuós em fità un instant', nbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que en este caso, el clasificador si que los diferencia correctamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Generación de texto\n",
    "\n",
    "Construya modelos n-gram con n=1, n=3, n=5 y=7 a partir de un texto elegido por usted. Puede ser una obra del Proyecto Gutenberg o una noticia de prensa, por ejemplo. Observe como medida que aumenta el número de palabras que se han tenido en cuenta la verosimilitud del texto generado es mayor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngram: 1\n",
      "hablando\n",
      "\n",
      "\n",
      "Ngram: 2\n",
      "am rico en tiempo limitado mengo barrildo v tor mengo de alemania que reyes cristi nigos y quebrantando su se precian casados que note images generously made available for any volunteers and distribution of receiving it licas se siembre o vu lvete si el siglo de masa echando hasta que serlo ninguno pascuala tienes teni ndome a aqu descansaba un poeta componiendo la biblioteca de chinas c rtale la forma anticuada de enorme juan la hermosa fiera inhumano mengo cara tiene el sacrist n soldadesca a cierto mengo para tu esposo amado nueva me sigue un valeroso soldado maestre don\n",
      "\n",
      "\n",
      "Ngram: 3\n",
      "cierto plat n que por mi honor y agrad zcole a tu amor la procura laurencia para gozarla mengo eso creo pues ese gusto que intenta no es puesto en el peligro presente y de tu crueldad a la justicia divina ll vanla y vanse y salen las mujeres armadas laurencia parad en este puesto de esperanzas soldados atrevidos no mujeres pascuala qu es ser amado comendador la puerta de mi encomienda escog casa entre aquestas mudanzas vuestra gente se registre que no escucha palabras el d a sola una honda ten a deseado manrique mi fe que son extremadas\n",
      "\n",
      "\n",
      "Ngram: 4\n",
      "hagan un tiempo el que 24 despu s y antes y pidiendo el presente lo importante al m s sabio ver is m s que me contaron que a la saz n representaba la uniformidad de la ley un ideal m s amplio una mayor garant a de justicia para el pueblo de esta suerte el drama es de una profunda l gica dentro de la historia los vecinos de la villa a su se or a comendador desdenes el otro d a quien trate de alzar pend n contra l pues ya sab is lo que responder ten is\n",
      "\n",
      "\n",
      "Ngram: 5\n",
      "castigo pascuala vengar tus azotes mengo eso digo jacinta ea muera el traidor flores piedad se oras sale ortu o huyendo de laurencia ortu o mira que no soy yo el culpado mengo cuando ser alcahuete no bastara bastaba haberme el p caro azotado pascuala d noslo a las mujeres mengo para acaba por tu vida mengo ya est dado que no le quiero yo mayor castigo pascuala vengar tus azotes mengo eso digo jacinta ea muera el traidor flores piedad se oras sale ortu o huyendo de laurencia ortu o mira que no soy yo laurencia ya s qui\n",
      "\n",
      "\n",
      "Ngram: 6\n",
      "cosa pascuala en que yo soy parte es esto contra el maestre t llez gir n que dios guarde es contra toda su orden es su honor y es importante para el ejemplo el castigo que habr otro d a quien trate de alzar pend n contra l pues ya sab is que una tarde al comendador mayor qu vasallos tan leales puso una ballesta al pecho esteban supuesto que el disculparle ya puede tocar a un suegro no es mucho que en causas tales se descomponga con vos un hombre en efecto amante porque si vos pretend is su\n",
      "\n",
      "\n",
      "Ngram: 7\n",
      "otro m s riguroso y peor vocabulario en las lenguas descorteses frondoso querr a que lo dijeses laurencia es todo a esotro contrario al hombre grave enfadoso venturoso al descompuesto 10 melanc lico al compuesto y al que reprehende odioso importuno al que aconseja al liberal moscatel 11 al justiciero cruel y al que es piadoso madeja 12 al que es constante villano al que es cort s lisonjero hip crita al limosnero y pretendiente al cristiano al justo m rito dicha a la verdad imprudencia cobard a a la paciencia y culpa a lo que es desdicha necia a\n",
      "\n",
      "\n",
      "Ngram: 8\n",
      "sus trojes vean de rubio trigo en agosto atestadas y colmadas y sus tinajas de mosto que tal imaginaci n me ha llegado a dar enojo ni me desvela ni aflige ni en ella el cuidado pongo 19 instrumento de m sica llamado tambi n baj n frondoso tal me tienen tus desdenes bella laurencia que tomo en el peligro de verte la vida cuando te oigo si sabes que es mi intenci n el desear ser tu esposo mal premio das a mi fe laurencia es que yo no s dar otro frondoso posible es que no te duelas\n",
      "\n",
      "\n",
      "Ngram: 9\n",
      "en la traza del dote qu le diremos que yo bien te puedo dar cuatro mil maraved s frondoso se or eso me dec s mi honor quer is agraviar esteban anda hijo que eso es cosa que pasa en un d a que si no hay dote a fe m a que se echa menos despu s vanse y quedan frondoso y laurencia laurencia di frondoso est s contento frondoso c mo si lo estoy es poco pues que no me vuelvo loco de gozo del bien que siento risa vierte el coraz n por los ojos de alegr\n",
      "\n",
      "\n",
      "Ngram: 10\n",
      "quitado en llevarme para ti comendador en quererte llevar jacinta s porque tengo un padre honrado que si en alto nacimiento no te iguala en las costumbres te vence comendador las pesadumbres y el villano atrevimiento no tiemplan bien un airado tira por ah jacinta con qui n comendador conmigo jacinta m ralo bien comendador para tu mal lo he mirado ya no m a del bagaje del ej rcito has de ser jacinta no tiene el mundo poder para hacerme viva ultraje comendador ea villana camina jacinta piedad se or comendador no hay piedad jacinta apelo de tu crueldad\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random import choice\n",
    "from aima_module.text import NgramWordModel, words\n",
    "\n",
    "def generate_text(model, length, ngram=5):\n",
    "\tgenerate = []\n",
    "\tseed = choice(list(model.dictionary.keys()))\n",
    "\tgenerate.extend(seed)\n",
    "\n",
    "\tfor i in range(length-ngram):\n",
    "\t\tlast = tuple(generate[-(ngram-1):])\n",
    "\t\tnext = []\n",
    "\n",
    "\t\tfor j in model.dictionary.keys():\n",
    "\t\t\tif j[:ngram-1] == last: next.append(j[ngram-1])\n",
    "\t\t\n",
    "\t\tif not next: break\n",
    "\n",
    "\t\tnext = choice(next)\n",
    "\t\tgenerate.append(next)\n",
    "\t\n",
    "\treturn ' '.join(generate)\n",
    "\n",
    "# In my case I decided to use the Spanish classic 'Fuenteovejuna' by Lope de Vega\n",
    "fuenteovejuna = open_data('ES-Text/fuente_ovejuna.txt').read()\n",
    "wordseq = words(fuenteovejuna)\n",
    "\n",
    "# I decided to automate the text generation process from 1 ngram to 10 ngrams and 100 words length\n",
    "for i in range(1, 11):\n",
    "\tmodel = NgramWordModel(i, wordseq)\n",
    "\tprint(f'Ngram: {i}\\n{generate_text(model=model, length=100, ngram=i)}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se esperaba, a medida que se aumentan los _n-gram_ los textos son más coherentes, aunque se pueden ver ciertos casos como en el **ngram=8** que no es perfecto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Análisis sintáctico\n",
    "\n",
    "Aplica las herramientas de alguna librería Python para analizar sintácticamente la siguiente oración:\n",
    "\n",
    "- (ES): _Todos los seres humanos nacen libres e iguales en dignidad y derechos._\n",
    "\n",
    "- (CA): _Tots els éssers humans neixen lliures i iguals en dignitat i drets._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ca-core-news-sm==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/ca_core_news_sm-3.8.0/ca_core_news_sm-3.8.0-py3-none-any.whl (19.6 MB)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ca_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download ca_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es-core-news-sm==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "es_nlp = spacy.load('es_core_news_sm')\n",
    "ca_nlp = spacy.load('ca_core_news_sm')\n",
    "\n",
    "es_sentence = 'Todos los seres humanos nacen libres e iguales en dignidad y derechos'\n",
    "ca_sentence = 'Tots els éssers humans neixen lliures i iguals en dignitat i drets'\n",
    "\n",
    "es_doc = es_nlp(es_sentence)\n",
    "ca_doc = ca_nlp(ca_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- CATALAN Syntax -\n",
      "Noun Phrases: ['Tots els éssers humans', 'dignitat i drets']\n",
      "Verbs: ['neixar']\n",
      "Subjects: ['éssers']\n",
      "Adjectives: ['humans', 'lliures', 'iguals']\n",
      "Prepositions: ['en']\n",
      "Conjunctions: ['i', 'i']\n",
      "+ Each word Information:\n",
      "Word: Tots | Lemma: tot | Part of Speech: DET | Syntactic Dependency: det | Head: els \n",
      "\n",
      "Word: els | Lemma: el | Part of Speech: DET | Syntactic Dependency: det | Head: éssers \n",
      "\n",
      "Word: éssers | Lemma: ésser | Part of Speech: NOUN | Syntactic Dependency: nsubj | Head: neixen \n",
      "\n",
      "Word: humans | Lemma: humà | Part of Speech: ADJ | Syntactic Dependency: amod | Head: éssers \n",
      "\n",
      "Word: neixen | Lemma: neixar | Part of Speech: VERB | Syntactic Dependency: ROOT | Head: neixen \n",
      "\n",
      "Word: lliures | Lemma: lliure | Part of Speech: ADJ | Syntactic Dependency: obj | Head: neixen \n",
      "\n",
      "Word: i | Lemma: i | Part of Speech: CCONJ | Syntactic Dependency: cc | Head: iguals \n",
      "\n",
      "Word: iguals | Lemma: igual | Part of Speech: ADJ | Syntactic Dependency: conj | Head: lliures \n",
      "\n",
      "Word: en | Lemma: en | Part of Speech: ADP | Syntactic Dependency: case | Head: dignitat \n",
      "\n",
      "Word: dignitat | Lemma: dignitat | Part of Speech: NOUN | Syntactic Dependency: nmod | Head: lliures \n",
      "\n",
      "Word: i | Lemma: i | Part of Speech: CCONJ | Syntactic Dependency: cc | Head: drets \n",
      "\n",
      "Word: drets | Lemma: dret | Part of Speech: NOUN | Syntactic Dependency: conj | Head: dignitat \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Catalan sentence syntax analysis\n",
    "print('- CATALAN Syntax -')\n",
    "print('Noun Phrases:', [chunk.text for chunk in ca_doc.noun_chunks])\n",
    "print('Verbs:', [token.lemma_ for token in ca_doc if token.pos_ == 'VERB'])\n",
    "print('Subjects:', [token.text for token in ca_doc if token.dep_ == 'nsubj'])\n",
    "print('Adjectives:', [token.text for token in ca_doc if token.pos_ == 'ADJ'])\n",
    "print('Prepositions:', [token.text for token in ca_doc if token.pos_ == 'ADP'])\n",
    "print('Conjunctions:', [token.text for token in ca_doc if token.pos_ == 'CCONJ'])\n",
    "print('+ Each word Information:')\n",
    "for token in ca_doc:\n",
    "\tprint(f'Word: {token.text} | Lemma: {token.lemma_} | Part of Speech: {token.pos_} | Syntactic Dependency: {token.dep_} | Head: {token.head.text} \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- SPANISH Syntax -\n",
      "Noun Phrases: ['Todos los seres humanos', 'dignidad', 'derechos']\n",
      "Verbs: ['nacer']\n",
      "Subjects: ['seres']\n",
      "Adjectives: ['humanos', 'libres', 'iguales']\n",
      "Prepositions: ['en']\n",
      "Conjunctions: ['e', 'y']\n",
      "+ Each word Information:\n",
      "Word: Todos | Lemma: todo | Part of Speech: DET | Syntactic Dependency: det | Head: los \n",
      "\n",
      "Word: los | Lemma: el | Part of Speech: DET | Syntactic Dependency: det | Head: seres \n",
      "\n",
      "Word: seres | Lemma: ser | Part of Speech: NOUN | Syntactic Dependency: nsubj | Head: nacen \n",
      "\n",
      "Word: humanos | Lemma: humano | Part of Speech: ADJ | Syntactic Dependency: amod | Head: seres \n",
      "\n",
      "Word: nacen | Lemma: nacer | Part of Speech: VERB | Syntactic Dependency: ROOT | Head: nacen \n",
      "\n",
      "Word: libres | Lemma: libre | Part of Speech: ADJ | Syntactic Dependency: obj | Head: nacen \n",
      "\n",
      "Word: e | Lemma: e | Part of Speech: CCONJ | Syntactic Dependency: cc | Head: iguales \n",
      "\n",
      "Word: iguales | Lemma: igual | Part of Speech: ADJ | Syntactic Dependency: conj | Head: libres \n",
      "\n",
      "Word: en | Lemma: en | Part of Speech: ADP | Syntactic Dependency: case | Head: dignidad \n",
      "\n",
      "Word: dignidad | Lemma: dignidad | Part of Speech: NOUN | Syntactic Dependency: nmod | Head: libres \n",
      "\n",
      "Word: y | Lemma: y | Part of Speech: CCONJ | Syntactic Dependency: cc | Head: derechos \n",
      "\n",
      "Word: derechos | Lemma: derecho | Part of Speech: NOUN | Syntactic Dependency: conj | Head: libres \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Spanish sencence syntax analysis\n",
    "print('- SPANISH Syntax -')\n",
    "print('Noun Phrases:', [chunk.text for chunk in es_doc.noun_chunks])\n",
    "print('Verbs:', [token.lemma_ for token in es_doc if token.pos_ == 'VERB'])\n",
    "print('Subjects:', [token.text for token in es_doc if token.dep_ == 'nsubj'])\n",
    "print('Adjectives:', [token.text for token in es_doc if token.pos_ == 'ADJ'])\n",
    "print('Prepositions:', [token.text for token in es_doc if token.pos_ == 'ADP'])\n",
    "print('Conjunctions:', [token.text for token in es_doc if token.pos_ == 'CCONJ'])\n",
    "print('+ Each word Information:')\n",
    "for token in es_doc:\n",
    "\tprint(f'Word: {token.text} | Lemma: {token.lemma_} | Part of Speech: {token.pos_} | Syntactic Dependency: {token.dep_} | Head: {token.head.text} \\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Recuperación de información\n",
    "\n",
    "Cargue el artículo de la wikipedia sobre Europa ([https://es.wikipedia.org/wiki/Europa](https://es.wikipedia.org/wiki/Europa)) en una lista de frases y recupera la información siguiente, buscando las frases más parecidas.\n",
    "\n",
    "- ¿Cuándo se gestó el concepto de Europa?\n",
    "\n",
    "- ¿Cuál es la especie humana autóctona de Europa?\n",
    "\n",
    "- ¿Cuándo se formaron los estados actuales de Europa?\n",
    "\n",
    "- ¿Qué clima tiene Europa?\n",
    "\n",
    "¿Todas las preguntas obtienen una respuesta adecuada? ¿Hay preguntas que conviene formular de otra forma?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: ¿Cuándo se gestó el concepto de Europa?\n",
      "Best Sentence: Vegeu-ne altres significats a «Europa (desambiguació)».\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Question 2: ¿Cuál es la especie humana autóctona de Europa?\n",
      "Best Sentence: Aquesta espècie es trobava ja a Europa quan va arribar l'humà de Cromanyó (Homo sapiens), espècie a què pertany tota la humanitat actual.\n",
      "\n",
      "\n",
      "Question 3: ¿Cuándo se formaron los estados actuales de Europa?\n",
      "Best Sentence: Molts dels estats de l'Europa actual es van formar després de la Primera Guerra Mundial.\n",
      "\n",
      "\n",
      "Question 4: ¿Qué clima tiene Europa?\n",
      "Best Sentence: [Consulta: 14 febrer 2011].\n",
      "\n",
      "↑ «European Climate».\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "response = get('https://ca.wikipedia.org/wiki/Europa')\n",
    "html = BeautifulSoup(response.text, 'html.parser')\n",
    "content = ' '.join([p.text for p in html.find_all([\"p\", \"div\"]) if p.text])\n",
    "\n",
    "nlp = spacy.load('ca_core_news_sm')\n",
    "document = nlp(content)\n",
    "sentences = [sentence.text for sentence in document.sents]\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "questions = [\n",
    "\t'¿Cuándo se gestó el concepto de Europa?',\n",
    "\t'¿Cuál es la especie humana autóctona de Europa?',\n",
    "\t'¿Cuándo se formaron los estados actuales de Europa?',\n",
    "\t'¿Qué clima tiene Europa?'\n",
    "]\n",
    "questions_embeddings = model.encode(questions, convert_to_tensor=True)\n",
    "sentences_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "for i, question in enumerate(questions):\n",
    "\tscores = util.pytorch_cos_sim(questions_embeddings[i], sentences_embeddings)\n",
    "\tbest_sentence = scores.argmax()\n",
    "\tprint(f'Question {i+1}: {question}\\nBest Sentence: {sentences[best_sentence]}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado puede ser un tanto interesante desde el punto de vista de lo triviales que pueden resultar algunas preguntas para el ser humano y lo contrario que se puede observar en el modelo que estamos tratando en esta práctica. La segunda y tercera preguntan tienen como resultados más posibles o frases mejor valoradas unas bastante coherentes aunque cabe mencionar que algo incompletas.\n",
    "\n",
    "Con lo que respecta a la primera y última el resultado es bastante impactante ya que no responden ni remotamente a la pregunta realizada. Ambas de hecho parecen ser referencias a otros artículos mencionados en el cargado en el modelo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
